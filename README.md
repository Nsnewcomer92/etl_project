# etl_project

# ETL Final Technical Report

Extract:

We decided to pull two datasets from Kaggle.com, both of which were non-relational. The first was a dataset called avocados that gave different data about the prices of avocados from 2015 to 2018. The second dataset was called Wine reviews. This dataset showed the different types/brands of wines across the world with reviews about each different wine. Since these are non-relational, we chose to use MongoDB to extract our data.  First, we downloaded the datasets from Kaggle and saved them as CSVs to our project repository.

Transform:

To transform the data we decided to use Jupyter Notebook.  The first thing that we did once we had imported pandas, csv and json was to read the CSV files into Pandas and create Data Frames.  We then began the cleaning phase.  We renamed a few of the headers in the avocado file so that it was easily understood by replacing the PLU numbers to the size of avocados.  We also dropped any N/A values and set the index to the id provided in the csv.  Next, we dropped a few columns in the wine file, dropped any N/A values, and set the index to the id provided in the csv.  We then saved these new Data Frames as CSVs to reference later. Since both of these datasets are non-relational, we chose to use MongoDB, which means we needed to convert the cleaned CSVs into JSON format.  After we finished transforming our original datasets, we were ready to load them into our database.

Load:

For the loading portion of our project we decided to go with the trusty MongoDB. Thanks to all of the other awesome teammates that I had I was able to get this data loaded into the DB with little to no issues. Now I am going to go into the step by step instructions on how even you (the reader) can load your own JSON data into a non-relational DB like MongoDB. The first thing that I did to get started was create a master directory that is going to house all of our data and script that is being used to insert the data into the DB. Within the root directory I have one folder called resources that holds all of our JSON data for both the wine data and the avocado data. Inside of the Resources folder I also created a new python (.py) file called mongo.py, this is where the main script is held to insert our transformed data into the DB. First open up the mongo.py and walk through how we managed to get our data loaded. The first thing I got out of the way were my imports, I imported the json module so I could handle reading and writing json. I also imported the MongoClient object from pymongo so I can create a connection to my mongodb and begin loading data to it. I then went through and created my connection to my client and DB using some built in functionality that pymongo comes with. The second to last thing that I do is read in my external JSON files and load that content into a variable that way it's extremely easy for me to just insert that data next with pymongo. Lastly I use the (insert_many) function that comes with pymongo and in the wine collection I insert the wine data and in the avocado collection I insert the avocado data, I used both the variables I created before when I loaded the data from the Resources folder to insert that data so it's super clean and easy code to read. That's really it, you can now go through and access all of that data if you want. You can use it on a Flask app or use it for some fun data analysis. The sky's the limit once you get the data loaded into the db in the proper format.
